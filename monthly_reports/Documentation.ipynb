{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API development for download of Sentinel-2 and Landsat-8 data\n",
    "### User defined mosaicing on harmonised products\n",
    "### Robin Kohrs & Konstantin Schellenberg, February 2020, GEO 419\n",
    "### Supervisor: John Truckenbord*, Martin Habermeyer**\n",
    "* Friedrich-Schiller-University Jena, chair of remote sensing\n",
    "** Deutsches Luft- und Raumfahrtszentrum, department of high frequency (?)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Two different multi-spectral optical satellite constellations deliver open data source for earth observation.\n",
    "However, this extensively huge and growing database is merely comparable without significant preprocession and \n",
    "alignment. New products from NASA promise an ease of efforts of comparability. The *H*armonised *L*andsat *S*entinel\n",
    "product (HLS) provides coregistered pixels and atmospherecly corrected, thus fully harmonsed scenes. By matching\n",
    "the individual swaths of the satellite constellations to the UTM tiles access is facilitated. \n",
    "<br>\n",
    "This work\n",
    "\n",
    "### The Product  \n",
    "The Sentinel-2 mission contains two identical satellites with suffix A and B, with opposite direction of orbits flying in a \n",
    "repeat cycle of 10 days each, 5 days for both orbits, respectively. Launch: 2015 (A), 2017 (B)[ESA].\n",
    "<br>\n",
    "As the second optical satellite Landsat 8 Operational Land Imager (OLI),...\n",
    "with a repeat cycle of 16 days.  Launch: 2013, come with [res. etc.]... 2013\n",
    "<br>\n",
    "\n",
    "\n",
    "The scope of this extension to the python-based nasa_hls[email/github] aims to add the following functionality:\n",
    "1. Download tiles of the harmonised product with user input geometry\n",
    "2. Spatial mosaicing of the products for Landsat and Sentinel, and the acquisition dates, respectively\n",
    "\n",
    "Additionally, we want to show that spectral indexes can be calculated with the downloaded products (3). \n",
    "<br>\n",
    "The results are plotted with `ipyleafet` and `folium` on the Open Street Map WMS service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download HSL files with user input\n",
    "First, the module nasa_hls is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nasa_hls\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# and for later processing in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For testing purpose, try downloading the kml file:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nasa_hls.download_kml()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make tiles from the user given:\n",
    "1. spatial geometry\n",
    "2. date or time span\n",
    "3. product type (S30 or L30, or both)\n",
    "\n",
    "The input features should come in the WGS84 coordinate system.\n",
    "Dates have to be passes as yyyy-mm-dd strings.\n",
    "(when REQUEST = GetFeature) The time or time range for which to return the results, in ISO8601 format (year-month-date, for example: 2016-01-01). When a single time is specified the service will return data until the specified time. If a time range is specified the result is based on all scenes between the specified dates conforming to the cloud coverage criteria and stacked based on priority setting - e.g. most recent on top. The time range is written as two time values separated by a slash, followed by a second slash and a period parameter (which must be P1D). Optional, default: none (the last valid image is returned). Examples: \"TIME=2016-01-01\", \"TIME=2016-01-01/2016-02-01/P1D\".\n",
    "https://www.sentinel-hub.com/develop/documentation/api/ogc_api/wfs-request"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds = nasa_hls.make_tiles_dataset(shape=\"/home/aleko-kon/Dokumente/nasa_hls/data/amazon.shp\",\n",
    "                                products=[\"S30\", \"L30\"],\n",
    "                                date=\"2018-05-05\")\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taking this list, the data sources can be downloaded via `download_tiles`. <br> This function call `download_batch` and other methods internally in order to parse the right URLs for download."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nasa_hls.download_tiles(dstdir=os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf'),\n",
    "                        datasets=ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Mosaicing tiles\n",
    "To simplify the handling of the downloaded files ... we mosaic the data according to the \n",
    "`make_mosaic.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_in = os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf' + os.sep)\n",
    "path_out =  os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'tifs' + os.sep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "from spatialist import Vector\n",
    "import rasterio\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from nasa_hls.download_tiles import get_available_datasets_from_shape\n",
    "from nasa_hls.download_hls_dataset import download_batch\n",
    "from nasa_hls.utils import BAND_NAMES\n",
    "\n",
    "def make_mosaic_tif(srcdir = path_in, dstdir = path_out, bands = None, product = \"S30\"):\n",
    "\n",
    "    # get all hdf-files\n",
    "    hdf_files_list = list(glob.glob(srcdir + '*.hdf'))\n",
    "\n",
    "    # make list of all dates in directory\n",
    "    dates_doy = []\n",
    "    for line in hdf_files_list:\n",
    "        l = line.split(\".\")[3][4:]\n",
    "        dates_doy.append(l)\n",
    "\n",
    "    # print list with unique\n",
    "    #print(dates_doy)\n",
    "\n",
    "    # make a function that gets the unique entries from a list\n",
    "    # these will be the keys afterwards\n",
    "    def unique_dates(liste):\n",
    "        unique_dates = []\n",
    "        for x in liste:\n",
    "            if x not in unique_dates:\n",
    "                unique_dates.append(x)\n",
    "        return unique_dates\n",
    "    \n",
    "    # make the list of unique dates\n",
    "    unique_doy = unique_dates(dates_doy)\n",
    "    \n",
    "\n",
    "    # create dictionary with keys being the unique dates\n",
    "    # not yet specify the value-type\n",
    "    dataframe_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # add rows of orignial dataframe as values\n",
    "    for key in dataframe_dict.keys():\n",
    "        foo = []\n",
    "        # now go over all the files\n",
    "        for line in hdf_files_list:\n",
    "            # get the doy\n",
    "            line_date = line.split(\".\")[3][4:]\n",
    "            # wenn doy in der line == dem key, dann schreib es in die liste foo\n",
    "            if key == line_date:\n",
    "                foo.append(line)\n",
    "        # nachdem du über alle files gegangen bist, schreib an den key mit dem doy die aktuelle foo-liste,\n",
    "        # die nach diesem Durchgang wieder neu aufgesetzt wird\n",
    "        dataframe_dict[key] = foo\n",
    "\n",
    "    #print(dataframe_dict[\"311\"], \"\\n\\n\")\n",
    "    #for key, item in dataframe_dict.items():\n",
    "         #print(key, item, \"\\n\")\n",
    "\n",
    "     #check if band is specified\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "        long_band_names = []\n",
    "        for long_band_name in bands:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "            long_band_names.append(band)\n",
    "    else:\n",
    "        long_band_names = bands\n",
    "        \n",
    "    \n",
    "\n",
    "    for key in dataframe_dict.keys():\n",
    "        for band in long_band_names:\n",
    "            hdf_list = dataframe_dict[key]\n",
    "            hdf_file_bands = []\n",
    "            for hdf_file in hdf_list:\n",
    "                filename = 'HDF4_EOS:EOS_GRID:\"{0}\":Grid:{1}'.format(hdf_file, band)\n",
    "                hdf_file_bands.append(filename)\n",
    "\n",
    "            #print(\"\\n\".join(hdf_file_bands))\n",
    "            # make mosaics for each band for each date\n",
    "            vrt_path = os.path.join(path_out + key + band + \".vrt\")\n",
    "            build_vrt = gdal.BuildVRT(vrt_path, hdf_file_bands)\n",
    "            build_vrt = None\n",
    "\n",
    "    dates_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # list of vrts\n",
    "    vrts_path = path_out\n",
    "    vrts = list(glob.glob(vrts_path + \"*.vrt\"))\n",
    "    \n",
    "    #print(vrts)\n",
    "    \n",
    "\n",
    "    for key in dates_dict.keys():\n",
    "        files = []\n",
    "        for single_file in vrts:\n",
    "            doy = single_file.split(\"/\")[-1][0:3]\n",
    "            if key == doy:\n",
    "                files.append(single_file)\n",
    "\n",
    "        dates_dict[key] = files\n",
    "\n",
    "\n",
    "    ######is\n",
    "    #print the dict\n",
    "    ######\n",
    "    # for keys, items in dates_dict.items():\n",
    "    #     print(keys, items, \"\\n\")\n",
    "    # print dictionary nicely\n",
    "    #print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in dates_dict.items()))\n",
    "    #print(len(dates_dict))\n",
    "\n",
    "    for date in dates_dict.keys():\n",
    "        print(date)\n",
    "        vrts_per_date = dates_dict[date]\n",
    "        vrt_path = os.path.join(path_out + date + \"final.vrt\")\n",
    "        single_vrt = gdal.BuildVRT(vrt_path, vrts_per_date, separate=True)\n",
    "        tiff_path = os.path.join(dstdir + date + \".tiff\")\n",
    "        final_tif = gdal.Translate(tiff_path, single_vrt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # final_tif = gdal.Translate(os.path.join(path_data_lin_robin + \"mosaic/\" + key + band + \".tiff\"), build_vrt)\n",
    "            # final_tif = None\n",
    "            # build_vrt = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_mosaic_tif()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output extent can now be clipped to the actual outline of the input geometry."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Indexes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "UTM tiles already successfully downloaded to:\n",
      " /home/aleko-kon/.nasa_hls/.auxdata/utm.kml \n",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "'/home/aleko-kon/.nasa_hls/.auxdata/utm.kml'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "nasa_hls.download_kml()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make tiles from the user given:\n",
    "1. spatial geometry\n",
    "2. date or time span\n",
    "3. product type (S30 or L30, or both)\n",
    "\n",
    "The input features should come in the WGS84 coordinate system.\n",
    "Dates have to be passes as yyyy-mm-dd strings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "single date: 2018-05-05\n",
      " \n",
      "UTM tiles already successfully downloaded to:\n",
      " /home/aleko-kon/.nasa_hls/.auxdata/utm.kml \n",
      "\n",
      "\n",
      "getting available datasets . . .\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f11de2691d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='hls.gsfc.nasa.gov', port=443): Max retries exceeded with url: /data/v1.4/S30/2018/19/L/D/J/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f11de2691d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7d377068e773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m ds = nasa_hls.make_tiles_dataset(shape=\"/home/aleko-kon/Dokumente/nasa_hls/data/amazon.shp\",\n\u001b[1;32m      2\u001b[0m                                 \u001b[0mproducts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"S30\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L30\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                 date=\"2018-05-05\")\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nasa_hls/nasa_hls/download_tiles.py\u001b[0m in \u001b[0;36mmake_tiles_dataset\u001b[0;34m(shape, products, date, year, start_date, end_date)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_datasets_from_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myyyy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;31m# SPLIT DATAFRAME AS USER DATE INPUT  . . .\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdates_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nasa_hls/nasa_hls/download_tiles.py\u001b[0m in \u001b[0;36mget_available_datasets_from_shape\u001b[0;34m(products, years, shape)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ngetting available datasets . . .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nasa_hls/nasa_hls/utils.py\u001b[0m in \u001b[0;36mget_available_datasets\u001b[0;34m(products, years, tiles, return_list)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{year}-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0murls_to_screen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_directories_in_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls_to_screen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.hdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_from_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nasa_hls/nasa_hls/utils.py\u001b[0m in \u001b[0;36m_get_directories_in_directories\u001b[0;34m(url_list, href_match)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0murls_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0murls_new\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_get_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0murls_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nasa_hls/nasa_hls/utils.py\u001b[0m in \u001b[0;36m_get_directories\u001b[0;34m(url, href_match)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='hls.gsfc.nasa.gov', port=443): Max retries exceeded with url: /data/v1.4/S30/2018/19/L/D/J/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f11de2691d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))"
     ],
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='hls.gsfc.nasa.gov', port=443): Max retries exceeded with url: /data/v1.4/S30/2018/19/L/D/J/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f11de2691d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))",
     "output_type": "error"
    }
   ],
   "source": [
    "ds = nasa_hls.make_tiles_dataset(shape=\"/home/aleko-kon/Dokumente/nasa_hls/data/amazon.shp\",\n",
    "                                products=[\"S30\", \"L30\"],\n",
    "                                date=\"2018-05-05\")\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taking this list, the data sources can be downloaded via `download_tiles`. <br> This function call `download_batch` and other methods internally in order to parse the right URLs for download."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nasa_hls.download_tiles(dstdir=os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf'),\n",
    "                        datasets=ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Mosaicing tiles\n",
    "To simplify the handling of the downloaded files ... we mosaic the data according to the \n",
    "`make_mosaic.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_in = os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf' + os.sep)\n",
    "path_out =  os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'tifs' + os.sep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "from spatialist import Vector\n",
    "import rasterio\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from nasa_hls.download_tiles import get_available_datasets_from_shape\n",
    "from nasa_hls.download_hls_dataset import download_batch\n",
    "from nasa_hls.utils import BAND_NAMES\n",
    "\n",
    "def make_mosaic_tif(srcdir = path_in, dstdir = path_out, bands = None, product = \"S30\"):\n",
    "\n",
    "    # get all hdf-files\n",
    "    hdf_files_list = list(glob.glob(srcdir + '*.hdf'))\n",
    "\n",
    "    # make list of all dates in directory\n",
    "    dates_doy = []\n",
    "    for line in hdf_files_list:\n",
    "        l = line.split(\".\")[3][4:]\n",
    "        dates_doy.append(l)\n",
    "\n",
    "    # print list with unique\n",
    "    #print(dates_doy)\n",
    "\n",
    "    # make a function that gets the unique entries from a list\n",
    "    # these will be the keys afterwards\n",
    "    def unique_dates(liste):\n",
    "        unique_dates = []\n",
    "        for x in liste:\n",
    "            if x not in unique_dates:\n",
    "                unique_dates.append(x)\n",
    "        return unique_dates\n",
    "    \n",
    "    # make the list of unique dates\n",
    "    unique_doy = unique_dates(dates_doy)\n",
    "    \n",
    "\n",
    "    # create dictionary with keys being the unique dates\n",
    "    # not yet specify the value-type\n",
    "    dataframe_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # add rows of orignial dataframe as values\n",
    "    for key in dataframe_dict.keys():\n",
    "        foo = []\n",
    "        # now go over all the files\n",
    "        for line in hdf_files_list:\n",
    "            # get the doy\n",
    "            line_date = line.split(\".\")[3][4:]\n",
    "            # wenn doy in der line == dem key, dann schreib es in die liste foo\n",
    "            if key == line_date:\n",
    "                foo.append(line)\n",
    "        # nachdem du über alle files gegangen bist, schreib an den key mit dem doy die aktuelle foo-liste,\n",
    "        # die nach diesem Durchgang wieder neu aufgesetzt wird\n",
    "        dataframe_dict[key] = foo\n",
    "\n",
    "    #print(dataframe_dict[\"311\"], \"\\n\\n\")\n",
    "    #for key, item in dataframe_dict.items():\n",
    "         #print(key, item, \"\\n\")\n",
    "\n",
    "     #check if band is specified\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "        long_band_names = []\n",
    "        for long_band_name in bands:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "            long_band_names.append(band)\n",
    "    else:\n",
    "        long_band_names = bands\n",
    "        \n",
    "    \n",
    "\n",
    "    for key in dataframe_dict.keys():\n",
    "        for band in long_band_names:\n",
    "            hdf_list = dataframe_dict[key]\n",
    "            hdf_file_bands = []\n",
    "            for hdf_file in hdf_list:\n",
    "                filename = 'HDF4_EOS:EOS_GRID:\"{0}\":Grid:{1}'.format(hdf_file, band)\n",
    "                hdf_file_bands.append(filename)\n",
    "\n",
    "            #print(\"\\n\".join(hdf_file_bands))\n",
    "            # make mosaics for each band for each date\n",
    "            vrt_path = os.path.join(path_out + key + band + \".vrt\")\n",
    "            build_vrt = gdal.BuildVRT(vrt_path, hdf_file_bands)\n",
    "            build_vrt = None\n",
    "\n",
    "    dates_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # list of vrts\n",
    "    vrts_path = path_out\n",
    "    vrts = list(glob.glob(vrts_path + \"*.vrt\"))\n",
    "    \n",
    "    #print(vrts)\n",
    "    \n",
    "\n",
    "    for key in dates_dict.keys():\n",
    "        files = []\n",
    "        for single_file in vrts:\n",
    "            doy = single_file.split(\"/\")[-1][0:3]\n",
    "            if key == doy:\n",
    "                files.append(single_file)\n",
    "\n",
    "        dates_dict[key] = files\n",
    "\n",
    "\n",
    "    ######is\n",
    "    #print the dict\n",
    "    ######\n",
    "    # for keys, items in dates_dict.items():\n",
    "    #     print(keys, items, \"\\n\")\n",
    "    # print dictionary nicely\n",
    "    #print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in dates_dict.items()))\n",
    "    #print(len(dates_dict))\n",
    "\n",
    "    for date in dates_dict.keys():\n",
    "        print(date)\n",
    "        vrts_per_date = dates_dict[date]\n",
    "        vrt_path = os.path.join(path_out + date + \"final.vrt\")\n",
    "        single_vrt = gdal.BuildVRT(vrt_path, vrts_per_date, separate=True)\n",
    "        tiff_path = os.path.join(dstdir + date + \".tiff\")\n",
    "        final_tif = gdal.Translate(tiff_path, single_vrt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # final_tif = gdal.Translate(os.path.join(path_data_lin_robin + \"mosaic/\" + key + band + \".tiff\"), build_vrt)\n",
    "            # final_tif = None\n",
    "            # build_vrt = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_mosaic_tif()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output extent can now be clipped to the actual outline of the input geometry."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Indexes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nasa_hls.download_kml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make tiles from the user given:\n",
    "1. spatial geometry\n",
    "2. date or time span\n",
    "3. product type (S30 or L30, or both)\n",
    "\n",
    "The input features should come in the WGS84 coordinate system.\n",
    "Dates have to be passes as yyyy-mm-dd strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ds = nasa_hls.make_tiles_dataset(shape=\"/home/aleko-kon/Dokumente/nasa_hls/data/amazon.shp\",\n",
    "                                products=[\"S30\", \"L30\"],\n",
    "                                date=\"2018-05-05\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this list, the data sources can be downloaded via `download_tiles`. <br> This function call `download_batch` and other methods internally in order to parse the right URLs for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nasa_hls.download_tiles(dstdir=os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf'),\n",
    "                        datasets=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Mosaicing tiles\n",
    "To simplify the handling of the downloaded files ... we mosaic the data according to the \n",
    "`make_mosaic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "path_in = os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf' + os.sep)\n",
    "path_out =  os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'tifs' + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "from spatialist import Vector\n",
    "import rasterio\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from nasa_hls.download_tiles import get_available_datasets_from_shape\n",
    "from nasa_hls.download_hls_dataset import download_batch\n",
    "from nasa_hls.utils import BAND_NAMES\n",
    "\n",
    "def make_mosaic_tif(srcdir = path_in, dstdir = path_out, bands = None, product = \"S30\"):\n",
    "\n",
    "    # get all hdf-files\n",
    "    hdf_files_list = list(glob.glob(srcdir + '*.hdf'))\n",
    "\n",
    "    # make list of all dates in directory\n",
    "    dates_doy = []\n",
    "    for line in hdf_files_list:\n",
    "        l = line.split(\".\")[3][4:]\n",
    "        dates_doy.append(l)\n",
    "\n",
    "    # print list with unique\n",
    "    #print(dates_doy)\n",
    "\n",
    "    # make a function that gets the unique entries from a list\n",
    "    # these will be the keys afterwards\n",
    "    def unique_dates(liste):\n",
    "        unique_dates = []\n",
    "        for x in liste:\n",
    "            if x not in unique_dates:\n",
    "                unique_dates.append(x)\n",
    "        return unique_dates\n",
    "    \n",
    "    # make the list of unique dates\n",
    "    unique_doy = unique_dates(dates_doy)\n",
    "    \n",
    "\n",
    "    # create dictionary with keys being the unique dates\n",
    "    # not yet specify the value-type\n",
    "    dataframe_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # add rows of orignial dataframe as values\n",
    "    for key in dataframe_dict.keys():\n",
    "        foo = []\n",
    "        # now go over all the files\n",
    "        for line in hdf_files_list:\n",
    "            # get the doy\n",
    "            line_date = line.split(\".\")[3][4:]\n",
    "            # wenn doy in der line == dem key, dann schreib es in die liste foo\n",
    "            if key == line_date:\n",
    "                foo.append(line)\n",
    "        # nachdem du über alle files gegangen bist, schreib an den key mit dem doy die aktuelle foo-liste,\n",
    "        # die nach diesem Durchgang wieder neu aufgesetzt wird\n",
    "        dataframe_dict[key] = foo\n",
    "\n",
    "    #print(dataframe_dict[\"311\"], \"\\n\\n\")\n",
    "    #for key, item in dataframe_dict.items():\n",
    "         #print(key, item, \"\\n\")\n",
    "\n",
    "     #check if band is specified\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "        long_band_names = []\n",
    "        for long_band_name in bands:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "            long_band_names.append(band)\n",
    "    else:\n",
    "        long_band_names = bands\n",
    "        \n",
    "    \n",
    "\n",
    "    for key in dataframe_dict.keys():\n",
    "        for band in long_band_names:\n",
    "            hdf_list = dataframe_dict[key]\n",
    "            hdf_file_bands = []\n",
    "            for hdf_file in hdf_list:\n",
    "                filename = 'HDF4_EOS:EOS_GRID:\"{0}\":Grid:{1}'.format(hdf_file, band)\n",
    "                hdf_file_bands.append(filename)\n",
    "\n",
    "            #print(\"\\n\".join(hdf_file_bands))\n",
    "            # make mosaics for each band for each date\n",
    "            vrt_path = os.path.join(path_out + key + band + \".vrt\")\n",
    "            build_vrt = gdal.BuildVRT(vrt_path, hdf_file_bands)\n",
    "            build_vrt = None\n",
    "\n",
    "    dates_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # list of vrts\n",
    "    vrts_path = path_out\n",
    "    vrts = list(glob.glob(vrts_path + \"*.vrt\"))\n",
    "    \n",
    "    #print(vrts)\n",
    "    \n",
    "\n",
    "    for key in dates_dict.keys():\n",
    "        files = []\n",
    "        for single_file in vrts:\n",
    "            doy = single_file.split(\"/\")[-1][0:3]\n",
    "            if key == doy:\n",
    "                files.append(single_file)\n",
    "\n",
    "        dates_dict[key] = files\n",
    "\n",
    "\n",
    "    ######is\n",
    "    #print the dict\n",
    "    ######\n",
    "    # for keys, items in dates_dict.items():\n",
    "    #     print(keys, items, \"\\n\")\n",
    "    # print dictionary nicely\n",
    "    #print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in dates_dict.items()))\n",
    "    #print(len(dates_dict))\n",
    "\n",
    "    for date in dates_dict.keys():\n",
    "        print(date)\n",
    "        vrts_per_date = dates_dict[date]\n",
    "        vrt_path = os.path.join(path_out + date + \"final.vrt\")\n",
    "        single_vrt = gdal.BuildVRT(vrt_path, vrts_per_date, separate=True)\n",
    "        tiff_path = os.path.join(dstdir + date + \".tiff\")\n",
    "        final_tif = gdal.Translate(tiff_path, single_vrt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # final_tif = gdal.Translate(os.path.join(path_data_lin_robin + \"mosaic/\" + key + band + \".tiff\"), build_vrt)\n",
    "            # final_tif = None\n",
    "            # build_vrt = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "make_mosaic_tif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output extent can now be clipped to the actual outline of the input geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}