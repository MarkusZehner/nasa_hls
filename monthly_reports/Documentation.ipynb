{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API development for download of Sentinel-2 and Landsat-8 data\n",
    "### User defined mosaicing on harmonised products\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Two different multi-spectral optical satellite constellations deliver open data source for earth observation.\n",
    "The Sentinel-2 mission contains two identical satellites with suffix A and B, with opposite direction of orbits flying in a \n",
    "repeat cycle of 10 days each, 5 days for both orbits, respectively. Launch: 2015 (A), 2017 (B)[ESA].\n",
    "<br>\n",
    "As the second optical satellite Landsat 8 Operational Land Imager (OLI),...\n",
    "with a repeat cycle of 16 days.  Launch: 2013, come with [res. etc.]... 2013\n",
    "<br>\n",
    "\n",
    "\n",
    "The scope of this extension to the python-based nasa_hls[email/github] aims to add the following functionality:\n",
    "1. Download tiles of the harmonised product with user input geometry\n",
    "2. Spatial mosaicing of the products for Landsat and Sentinel, and the acquisition dates, respectively\n",
    "\n",
    "Additionally, we want to show that spectral indexes can be calculated with the downloaded products (3). \n",
    "<br>\n",
    "The results are plotted with `ipyleafet` and `folium` on the Open Street Map WMS service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download HSL files with user input\n",
    "First, the module nasa_hls is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nasa_hls\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# and for later processing in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For testing purpose, try downloading the kml file:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nasa_hls.download_kml()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make tiles from the user given:\n",
    "1. spatial geometry\n",
    "2. date or time span\n",
    "3. product type (S30 or L30, or both)\n",
    "\n",
    "The input features should come in the WGS84 coordinate system.\n",
    "Dates have to be passes as yyyy-mm-dd strings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds = nasa_hls.make_tiles_dataset(shape=\"/home/aleko-kon/Dokumente/nasa_hls/data/amazon.shp\",\n",
    "                                products=[\"S30\", \"L30\"],\n",
    "                                date=\"2018-05-05\")\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taking this list, the data sources can be downloaded via `download_tiles`. <br> This function call `download_batch` and other methods internally in order to parse the right URLs for download."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nasa_hls.download_tiles(dstdir=os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf'),\n",
    "                        datasets=ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Mosaicing tiles\n",
    "To simplify the handling of the downloaded files ... we mosaic the data according to the \n",
    "`make_mosaic.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_in = os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf' + os.sep)\n",
    "path_out =  os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'tifs' + os.sep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "from spatialist import Vector\n",
    "import rasterio\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from nasa_hls.download_tiles import get_available_datasets_from_shape\n",
    "from nasa_hls.download_hls_dataset import download_batch\n",
    "from nasa_hls.utils import BAND_NAMES\n",
    "\n",
    "def make_mosaic_tif(srcdir = path_in, dstdir = path_out, bands = None, product = \"S30\"):\n",
    "\n",
    "    # get all hdf-files\n",
    "    hdf_files_list = list(glob.glob(srcdir + '*.hdf'))\n",
    "\n",
    "    # make list of all dates in directory\n",
    "    dates_doy = []\n",
    "    for line in hdf_files_list:\n",
    "        l = line.split(\".\")[3][4:]\n",
    "        dates_doy.append(l)\n",
    "\n",
    "    # print list with unique\n",
    "    #print(dates_doy)\n",
    "\n",
    "    # make a function that gets the unique entries from a list\n",
    "    # these will be the keys afterwards\n",
    "    def unique_dates(liste):\n",
    "        unique_dates = []\n",
    "        for x in liste:\n",
    "            if x not in unique_dates:\n",
    "                unique_dates.append(x)\n",
    "        return unique_dates\n",
    "    \n",
    "    # make the list of unique dates\n",
    "    unique_doy = unique_dates(dates_doy)\n",
    "    \n",
    "\n",
    "    # create dictionary with keys being the unique dates\n",
    "    # not yet specify the value-type\n",
    "    dataframe_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # add rows of orignial dataframe as values\n",
    "    for key in dataframe_dict.keys():\n",
    "        foo = []\n",
    "        # now go over all the files\n",
    "        for line in hdf_files_list:\n",
    "            # get the doy\n",
    "            line_date = line.split(\".\")[3][4:]\n",
    "            # wenn doy in der line == dem key, dann schreib es in die liste foo\n",
    "            if key == line_date:\n",
    "                foo.append(line)\n",
    "        # nachdem du über alle files gegangen bist, schreib an den key mit dem doy die aktuelle foo-liste,\n",
    "        # die nach diesem Durchgang wieder neu aufgesetzt wird\n",
    "        dataframe_dict[key] = foo\n",
    "\n",
    "    #print(dataframe_dict[\"311\"], \"\\n\\n\")\n",
    "    #for key, item in dataframe_dict.items():\n",
    "         #print(key, item, \"\\n\")\n",
    "\n",
    "     #check if band is specified\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "        long_band_names = []\n",
    "        for long_band_name in bands:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "            long_band_names.append(band)\n",
    "    else:\n",
    "        long_band_names = bands\n",
    "        \n",
    "    \n",
    "\n",
    "    for key in dataframe_dict.keys():\n",
    "        for band in long_band_names:\n",
    "            hdf_list = dataframe_dict[key]\n",
    "            hdf_file_bands = []\n",
    "            for hdf_file in hdf_list:\n",
    "                filename = 'HDF4_EOS:EOS_GRID:\"{0}\":Grid:{1}'.format(hdf_file, band)\n",
    "                hdf_file_bands.append(filename)\n",
    "\n",
    "            #print(\"\\n\".join(hdf_file_bands))\n",
    "            # make mosaics for each band for each date\n",
    "            vrt_path = os.path.join(path_out + key + band + \".vrt\")\n",
    "            build_vrt = gdal.BuildVRT(vrt_path, hdf_file_bands)\n",
    "            build_vrt = None\n",
    "\n",
    "    dates_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # list of vrts\n",
    "    vrts_path = path_out\n",
    "    vrts = list(glob.glob(vrts_path + \"*.vrt\"))\n",
    "    \n",
    "    #print(vrts)\n",
    "    \n",
    "\n",
    "    for key in dates_dict.keys():\n",
    "        files = []\n",
    "        for single_file in vrts:\n",
    "            doy = single_file.split(\"/\")[-1][0:3]\n",
    "            if key == doy:\n",
    "                files.append(single_file)\n",
    "\n",
    "        dates_dict[key] = files\n",
    "\n",
    "\n",
    "    ######is\n",
    "    #print the dict\n",
    "    ######\n",
    "    # for keys, items in dates_dict.items():\n",
    "    #     print(keys, items, \"\\n\")\n",
    "    # print dictionary nicely\n",
    "    #print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in dates_dict.items()))\n",
    "    #print(len(dates_dict))\n",
    "\n",
    "    for date in dates_dict.keys():\n",
    "        print(date)\n",
    "        vrts_per_date = dates_dict[date]\n",
    "        vrt_path = os.path.join(path_out + date + \"final.vrt\")\n",
    "        single_vrt = gdal.BuildVRT(vrt_path, vrts_per_date, separate=True)\n",
    "        tiff_path = os.path.join(dstdir + date + \".tiff\")\n",
    "        final_tif = gdal.Translate(tiff_path, single_vrt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # final_tif = gdal.Translate(os.path.join(path_data_lin_robin + \"mosaic/\" + key + band + \".tiff\"), build_vrt)\n",
    "            # final_tif = None\n",
    "            # build_vrt = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_mosaic_tif()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output extent can now be clipped to the actual outline of the input geometry."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Indexes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTM tiles already successfully downloaded to:\n",
      " /home/aleko-kon/.nasa_hls/.auxdata/utm.kml \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/aleko-kon/.nasa_hls/.auxdata/utm.kml'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_hls.download_kml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make tiles from the user given:\n",
    "1. spatial geometry\n",
    "2. date or time span\n",
    "3. product type (S30 or L30, or both)\n",
    "\n",
    "The input features should come in the WGS84 coordinate system.\n",
    "Dates have to be passes as yyyy-mm-dd strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid shape, process continues\n",
      "\n",
      "single date: 2018-05-05\n",
      " \n",
      "valid shape, process continues\n",
      "\n",
      "UTM tiles already successfully downloaded to:\n",
      " /home/aleko-kon/.nasa_hls/.auxdata/utm.kml \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting available datasets . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[    product   tile       date                                                url\n",
       " 40      S30  19LDJ 2018-05-05  https://hls.gsfc.nasa.gov/data/v1.4/S30/2018/1...\n",
       " 237     S30  19LEK 2018-05-05  https://hls.gsfc.nasa.gov/data/v1.4/S30/2018/1...\n",
       " 367     S30  19LDH 2018-05-05  https://hls.gsfc.nasa.gov/data/v1.4/S30/2018/1...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = nasa_hls.make_tiles_dataset(shape=\"/home/aleko-kon/Dokumente/nasa_hls/data/amazon.shp\",\n",
    "                                products=[\"S30\", \"L30\"],\n",
    "                                date=\"2018-05-05\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this list, the data sources can be downloaded via `download_tiles`. <br> This function call `download_batch` and other methods internally in order to parse the right URLs for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:29<02:58, 89.28s/it]"
     ]
    }
   ],
   "source": [
    "nasa_hls.download_tiles(dstdir=os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf'),\n",
    "                        datasets=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Mosaicing tiles\n",
    "To simplify the handling of the downloaded files ... we mosaic the data according to the \n",
    "`make_mosaic.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_in = os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'hdf' + os.sep)\n",
    "path_out =  os.path.join(os.path.expanduser('~'), 'Dokumente', 'nasa_hls', 'data', 'tifs' + os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "from spatialist import Vector\n",
    "import rasterio\n",
    "import glob\n",
    "from osgeo import gdal\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from nasa_hls.download_tiles import get_available_datasets_from_shape\n",
    "from nasa_hls.download_hls_dataset import download_batch\n",
    "from nasa_hls.utils import BAND_NAMES\n",
    "\n",
    "def make_mosaic_tif(srcdir = path_in, dstdir = path_out, bands = None, product = \"S30\"):\n",
    "\n",
    "    # get all hdf-files\n",
    "    hdf_files_list = list(glob.glob(srcdir + '*.hdf'))\n",
    "\n",
    "    # make list of all dates in directory\n",
    "    dates_doy = []\n",
    "    for line in hdf_files_list:\n",
    "        l = line.split(\".\")[3][4:]\n",
    "        dates_doy.append(l)\n",
    "\n",
    "    # print list with unique\n",
    "    #print(dates_doy)\n",
    "\n",
    "    # make a function that gets the unique entries from a list\n",
    "    # these will be the keys afterwards\n",
    "    def unique_dates(liste):\n",
    "        unique_dates = []\n",
    "        for x in liste:\n",
    "            if x not in unique_dates:\n",
    "                unique_dates.append(x)\n",
    "        return unique_dates\n",
    "    \n",
    "    # make the list of unique dates\n",
    "    unique_doy = unique_dates(dates_doy)\n",
    "    \n",
    "\n",
    "    # create dictionary with keys being the unique dates\n",
    "    # not yet specify the value-type\n",
    "    dataframe_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # add rows of orignial dataframe as values\n",
    "    for key in dataframe_dict.keys():\n",
    "        foo = []\n",
    "        # now go over all the files\n",
    "        for line in hdf_files_list:\n",
    "            # get the doy\n",
    "            line_date = line.split(\".\")[3][4:]\n",
    "            # wenn doy in der line == dem key, dann schreib es in die liste foo\n",
    "            if key == line_date:\n",
    "                foo.append(line)\n",
    "        # nachdem du über alle files gegangen bist, schreib an den key mit dem doy die aktuelle foo-liste,\n",
    "        # die nach diesem Durchgang wieder neu aufgesetzt wird\n",
    "        dataframe_dict[key] = foo\n",
    "\n",
    "    #print(dataframe_dict[\"311\"], \"\\n\\n\")\n",
    "    #for key, item in dataframe_dict.items():\n",
    "         #print(key, item, \"\\n\")\n",
    "\n",
    "     #check if band is specified\n",
    "    if bands is None:\n",
    "        bands = list(BAND_NAMES[product].keys())\n",
    "        long_band_names = []\n",
    "        for long_band_name in bands:\n",
    "            band = BAND_NAMES[product][long_band_name]\n",
    "            long_band_names.append(band)\n",
    "    else:\n",
    "        long_band_names = bands\n",
    "        \n",
    "    \n",
    "\n",
    "    for key in dataframe_dict.keys():\n",
    "        for band in long_band_names:\n",
    "            hdf_list = dataframe_dict[key]\n",
    "            hdf_file_bands = []\n",
    "            for hdf_file in hdf_list:\n",
    "                filename = 'HDF4_EOS:EOS_GRID:\"{0}\":Grid:{1}'.format(hdf_file, band)\n",
    "                hdf_file_bands.append(filename)\n",
    "\n",
    "            #print(\"\\n\".join(hdf_file_bands))\n",
    "            # make mosaics for each band for each date\n",
    "            vrt_path = os.path.join(path_out + key + band + \".vrt\")\n",
    "            build_vrt = gdal.BuildVRT(vrt_path, hdf_file_bands)\n",
    "            build_vrt = None\n",
    "\n",
    "    dates_dict = {date: None for date in unique_doy}\n",
    "\n",
    "    # list of vrts\n",
    "    vrts_path = path_out\n",
    "    vrts = list(glob.glob(vrts_path + \"*.vrt\"))\n",
    "    \n",
    "    #print(vrts)\n",
    "    \n",
    "\n",
    "    for key in dates_dict.keys():\n",
    "        files = []\n",
    "        for single_file in vrts:\n",
    "            doy = single_file.split(\"/\")[-1][0:3]\n",
    "            if key == doy:\n",
    "                files.append(single_file)\n",
    "\n",
    "        dates_dict[key] = files\n",
    "\n",
    "\n",
    "    ######is\n",
    "    #print the dict\n",
    "    ######\n",
    "    # for keys, items in dates_dict.items():\n",
    "    #     print(keys, items, \"\\n\")\n",
    "    # print dictionary nicely\n",
    "    #print(\"\\n\".join(\"{}\\t{}\".format(k, v) for k, v in dates_dict.items()))\n",
    "    #print(len(dates_dict))\n",
    "\n",
    "    for date in dates_dict.keys():\n",
    "        print(date)\n",
    "        vrts_per_date = dates_dict[date]\n",
    "        vrt_path = os.path.join(path_out + date + \"final.vrt\")\n",
    "        single_vrt = gdal.BuildVRT(vrt_path, vrts_per_date, separate=True)\n",
    "        tiff_path = os.path.join(dstdir + date + \".tiff\")\n",
    "        final_tif = gdal.Translate(tiff_path, single_vrt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # final_tif = gdal.Translate(os.path.join(path_data_lin_robin + \"mosaic/\" + key + band + \".tiff\"), build_vrt)\n",
    "            # final_tif = None\n",
    "            # build_vrt = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function TranslateInternal> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0m__swig_setmethods__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__swig_setmethods__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9c7959e625b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_mosaic_tif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-1eaabeb560eb>\u001b[0m in \u001b[0;36mmake_mosaic_tif\u001b[0;34m(srcdir, dstdir, bands, product)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0msingle_vrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildVRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvrt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvrts_per_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtiff_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdstdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tiff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mfinal_tif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiff_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_vrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mTranslate\u001b[0;34m(destName, srcDS, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0msrcDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTranslateInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrcDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m def WarpOptions(options = [], format = None,\n",
      "\u001b[0;32m~/miniconda3/envs/nasa-hls/lib/python3.7/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mTranslateInternal\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTranslateInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[0;34m\"\"\"TranslateInternal(char const * dest, Dataset dataset, GDALTranslateOptions translateOptions, GDALProgressFunc callback=0, void * callback_data=None) -> Dataset\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTranslateInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3161\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGDALWarpAppOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m     \u001b[0;34m\"\"\"Proxy of C++ GDALWarpAppOptions class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function TranslateInternal> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "make_mosaic_tif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output extent can now be clipped to the actual outline of the input geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualise Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}